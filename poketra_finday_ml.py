# -*- coding: utf-8 -*-
"""Copy of poketra-finday-ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10GmdhUeTOLAZUSee3XjBiQ2DiEH0b6M_
"""

# Installation de XGBoost (souvent d√©j√† install√© dans Colab, mais c'est une bonne pratique)
#!pip install xgboost

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, confusion_matrix, classification_report

print("Biblioth√®ques import√©es avec succ√®s.")

"""√âtape 1 : Chargement des Donn√©es et Inspection Rapide"""

try:
    train_df = pd.read_csv("ressources/train.csv")
    test_df = pd.read_csv("ressources/test.csv")

    # Affichage des premi√®res lignes pour v√©rifier le chargement
    print("--- TRAIN SET ---")
    print(train_df.head())
    print("\n--- TEST SET ---")
    print(test_df.head())

    # V√©rification du d√©s√©quilibre (l'indice crucial de la d√©tection de fraude)
    fraud_rate = train_df['is_fraud'].mean() * 100
    print(f"\nTaux de fraude dans le train set : {fraud_rate:.4f}% (D√©s√©quilibre S√âV√àRE!)")

except FileNotFoundError:
    print("ERREUR : Fichiers 'train.csv' ou 'test.csv' introuvables. V√©rifiez le nom et le chemin d'acc√®s.")

"""√âtape 2 : Feature Engineering"""

def create_temporal_features(df):
    """Cr√©ation des features temporelles (jour, heure, nuit, weekend) √† partir de 'step'."""

    # Le Step 1 est la premi√®re heure d'un LUNDI (jour 0)
    df['day_of_week'] = ((df['step'] - 1) // 24) % 7 # 0=Lundi, 6=Dimanche
    df['hour'] = (df['step'] - 1) % 24 # 0 √† 23

    # 1 si la transaction a lieu la nuit (ex: 0h √† 6h)
    df['is_night_time'] = ((df['hour'] >= 0) & (df['hour'] <= 6)).astype(int)
    # 1 si la transaction a lieu le week-end (Samedi ou Dimanche)
    df['is_weekend'] = ((df['day_of_week'] >= 5)).astype(int)

    return df

def advanced_feature_engineering(df):
    """Cr√©ation des features bas√©es sur le comportement du client (agr√©gations)."""

    # Fr√©quence des transactions : Combien de fois le client a transig√© dans cette fen√™tre de 24h
    df['step_24h_window'] = df['step'] // 24
    df['client_txn_count_24h'] = df.groupby(['customer_id', 'step_24h_window'])['amount'].transform('count')

    # Ratio d'anomalie : Montant de la transaction actuelle VS Montant moyen habituel pour ce type de transaction par ce client
    mean_amount_by_client_type = df.groupby(['customer_id', 'type'])['amount'].transform('mean')
    df['amount_vs_mean_type'] = df['amount'] / (mean_amount_by_client_type + 1e-6) # Ajout d'epsilon pour √©viter la division par z√©ro

    # Indicateur pour les types √† haut risque
    df['is_transfer_or_cashout'] = df['type'].apply(lambda x: 1 if x in ['TRANSFER', 'CASH_OUT'] else 0)

    # Suppression de la colonne temporaire
    df = df.drop('step_24h_window', axis=1)

    return df

# Application des fonctions √† l'entra√Ænement et au test
train_df = create_temporal_features(train_df.copy())
train_df = advanced_feature_engineering(train_df)

test_df_submission = test_df.copy() # Sauvegarde pour la soumission
test_df = create_temporal_features(test_df.copy())
test_df = advanced_feature_engineering(test_df)

# S√©paration de la cible (y) et des features (X)
X = train_df.drop(['is_fraud', 'transaction_id', 'customer_id', 'step'], axis=1)
y = train_df['is_fraud']
X_test_final = test_df.drop(['transaction_id', 'customer_id', 'step'], axis=1)

# Division pour la validation (on garde 20% du train pour tester le mod√®le)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("‚úÖ Feature Engineering termin√© et donn√©es s√©par√©es.")

"""√âtape 3 : Pr√©processeur et Baseline (R√©gression Logistique)"""

# --- D√©finition des Features pour le Pr√©processeur ---
numerical_features = ['amount', 'age', 'client_txn_count_24h', 'amount_vs_mean_type']
categorical_features = ['type', 'day_of_week', 'hour']
boolean_features = ['is_night_time', 'is_weekend', 'is_transfer_or_cashout']

# ColumnTransformer : Applique les bonnes transformations aux bonnes colonnes
preprocessor = ColumnTransformer(
    transformers=[
        # StandardScaler : Met les colonnes num√©riques √† la m√™me √©chelle (important pour la R√©g. Log.)
        ('num', StandardScaler(), numerical_features),
        # OneHotEncoder : Convertit les variables cat√©gorielles (ex: 'type') en colonnes num√©riques (0 ou 1)
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
        ('bool', 'passthrough', boolean_features) # Laisse les 0/1 tranquilles
    ],
    remainder='drop'
)

# --- Pipeline de Baseline (R√©gression Logistique) ---
log_reg_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    # LogisticRegression avec class_weight='balanced' pour g√©rer le d√©s√©quilibre
    ('classifier', LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear', max_iter=1000))
])

# Entra√Ænement et √âvaluation de la Baseline
print("üîÑ Entra√Ænement de la Baseline (R√©g. Logistique)...")
log_reg_model.fit(X_train, y_train)

y_pred_baseline = log_reg_model.predict(X_val)

print("\n--- R√©sultats de la Baseline ---")
f1_baseline = f1_score(y_val, y_pred_baseline)
print(f"F1-Score Baseline: **{f1_baseline:.4f}**")
print("\nMatrice de Confusion (Baseline) :\n", confusion_matrix(y_val, y_pred_baseline))
print("\nRapport de Classification (Baseline) :\n", classification_report(y_val, y_pred_baseline))

"""√âtape 4 : Mod√®le Avanc√© (XGBoost) et Optimisation"""

# --- Calcul du poids pour la classe positive (Gestion du D√©s√©quilibre pour XGBoost) ---
# On dit √† XGBoost que la classe '1' (fraude) est beaucoup plus importante
scale_pos_weight_value = sum(y_train == 0) / sum(y_train == 1)

# --- Pipeline XGBoost ---
xgb_classifier = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    n_estimators=200, # Un peu plus d'arbres
    learning_rate=0.08, # Un taux d'apprentissage mod√©r√©
    scale_pos_weight=scale_pos_weight_value, # Appliquer le poids calcul√©
    random_state=42,
    n_jobs=-1,
    use_label_encoder=False
)

xgb_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', xgb_classifier)
])

# Entra√Ænement du Mod√®le Final
print("\nüîÑ Entra√Ænement du Mod√®le Final (XGBoost)...")
xgb_model.fit(X_train, y_train)

# √âvaluation du Mod√®le Final
y_pred_xgb = xgb_model.predict(X_val)

print("\n--- R√©sultats du Mod√®le Final (XGBoost) ---")
f1_xgb = f1_score(y_val, y_pred_xgb)
print(f"F1-Score Final (XGBoost): **{f1_xgb:.4f}**")
print("\nMatrice de Confusion (XGBoost) :\n", confusion_matrix(y_val, y_pred_xgb))
print("\nRapport de Classification (XGBoost) :\n", classification_report(y_val, y_pred_xgb))

"""√âtape 5 : G√©n√©ration de la Soumission Finale"""

# 1. Entra√Ænement final sur l'ensemble des donn√©es d'entra√Ænement (X complet, y complet)
print("\nüîÑ Entra√Ænement final du XGBoost sur l'int√©gralit√© du TRAIN SET...")
xgb_model.fit(X, y)

# 2. Pr√©dictions sur le TEST SET
print("üöÄ G√©n√©ration des pr√©dictions pour la soumission...")
test_predictions = xgb_model.predict(X_test_final)

# 3. Cr√©ation du DataFrame de soumission
submission = pd.DataFrame({
    # On utilise les IDs originaux du test set
    "transaction_id": test_df_submission["transaction_id"],
    # La pr√©diction est convertie en entier (0 ou 1)
    "is_fraud": test_predictions.astype(int)
})

# 4. Sauvegarde du fichier submission.csv
submission.to_csv("submission.csv", index=False)

print(f"\n‚úÖ Fichier submission.csv g√©n√©r√© avec {len(submission)} pr√©dictions. Vous pouvez le t√©l√©charger dans l'onglet 'Fichiers' de Colab.")